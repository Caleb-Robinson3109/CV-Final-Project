\section{Datasets}
\label{sec:datasets}

From the KITRO project repository's setup instructions, two distinct datasets are provided, and KITRO runs on both datasets to refine them, effectively making them preprocessed datasets. The first dataset is 3DPW. The 3DPW dataset has many outdoor images tailored for 3D pose and shape estimation. The second dataset, Human3.6M, is an indoor 3D mesh dataset. The preprocessing for the regression model down the line is done by running KITRO, which converts both datasets into a common format. The converted format is a Python dictionary with the following values: image name, predicted 3D rotation matrix, predicted body shape parameters, predicted camera translations, intrinsic camera parameters, 2D keypoints, ground truth values of 3D rotation parameters, and ground truth values of body shape parameters.

For our project's time constraints, we had to further transform the preprocessed data in order to train the regression model on the data in a reasonable amount of time. Our first measure was to combine the two datasets into one overall dataset, and then take samples as large as one-hundredth of the new dataset. This was done because of hardware memory limits that prevented the entire dataset from being processed as one whole object. After that, we needed to run the KITRO model on the smaller samples.

Running KITRO generates the values for refined thetas, refined shape, refined camera, and refined vertices. The refined thetas, refined shape, and refined camera are input parameters used by SMPL, whereas the refined vertices are the output values given from SMPL used to represent a 3D mesh. These are stored in a PyTorch data file as a Python dictionary with correspondingly labeled stacks of tensors.

%With 100 samples of the original given data, as well as the KITRO refinements applied to them, the next step was to combine the samples to create one new dataset. Due to a lack of computer memory, five separate datasets were created. To distribute the datasets among the five final datasets, every fifth dataset was combined. With five datasets, four were designated as training datasets and one as a testing dataset.

The final step before training the regression model is to load the PyTorch data and represent the stacks of tensors as Numpy arrays. This is needed because most implementations of regression models in Python are designed for Numpy arrays, including scikit-learn's Random Forest regression model. The refined thetas, refined shape, and refined camera features are sorted into one Numpy array as the dependent (y) values, and all other features are sorted into another Numpy array as the independent (X) values; the name field is dropped in the process of converting the tensors to Numpy arrays.

Various other combinations of feature columns were also tested for the purposes of training, running, and evaluating the regression model. For example, in one trial, the refined vertices features were completely omitted from the set of independent (X) values. In another trial, the ground truth values for both pose and beta features were moved from the set of independent (X) values to the set of dependent (y) values. After visually inspecting each output's 3D mesh image, we decided on the current combination of independent and dependent values.